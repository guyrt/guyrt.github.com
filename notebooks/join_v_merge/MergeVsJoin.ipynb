{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to an Incremental Pipeline in Delta Lake: Change Tracking\n",
    "================================\n",
    "\n",
    "This post shows how to use Change Tracking (todo link) in Delta Lake 2.0 to convert a batch pipeline to an incremental update pipeline. We'll cover two parts:\n",
    "\n",
    "1. Capturing change tracking in a Delta Lake Merge job.\n",
    "1. Converting a series of `join` operations to `merge` operations to produce a cheaper pipeline using incremental operations.\n",
    "\n",
    "Setting Up a Scenario: 3 Tables\n",
    "--------------------------------------\n",
    "\n",
    "I've set up three tables:\n",
    "\n",
    "1. Invoice\n",
    "2. InvoiceItem\n",
    "3. Product\n",
    "\n",
    "The ground truth for these tables lives in a production system and is dumped to the data lake and merged into a delta lake table. The logic for this merge is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaChangeFeedExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")  \\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "    \n",
    "\n",
    "sc = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 0: Read the data and merge. This is just to get our tables set up. See Day 1 for a \"normal\" day.\n",
    "products = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/products/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "products.write.format(\"delta\").save(\"./outputs/products\")\n",
    "\n",
    "invoices = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoice/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoices.write.format(\"delta\").save(\"./outputs/invoices\")\n",
    "\n",
    "invoiceitems = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoiceitems/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoiceitems.write.format(\"delta\").save(\"./outputs/invoiceitems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New, assume there is a job that produces a normalized copy of the data that merges all three tables together. This data has one row per invoice item. We can perform normalization using a couple of joins. Occasionally we see \"hiccups\" where an invoice and invoice item exist in our data lake but the product has not yet been downloaded. This kind of delay can happen when tables are joined that come from different production systems. So, we'll left join products because they will occasionally be null. Bad things happen in complicated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build normalized view on day 0\n",
    "# build normalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "normalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "normalized_view = normalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").save(\"./outputs/normalized\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").save(\"./outputs/normalized_copy\")  # we'll use this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the price for these invoice items. We'll revisit them after day 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product p_16 on day=0 had price 51.18\n",
      "Prices in the combined field are all:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0f69b5dd-b7e8-445a-8268-f317e4edc2bd</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dd96153-630b-447c-b438-9047586e25fe</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f962845-105a-46c5-b835-0457eba2f996</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27f5a1f6-83c7-4b59-82de-70108b5b3d27</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2e9a0213-1a2a-4a40-88de-79e07becbcb7</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        invoice_item_id invoice_modified  price\n",
       "0  0f69b5dd-b7e8-445a-8268-f317e4edc2bd             day0  51.18\n",
       "1  1dd96153-630b-447c-b438-9047586e25fe             day0  51.18\n",
       "2  1f962845-105a-46c5-b835-0457eba2f996             day0  51.18\n",
       "3  27f5a1f6-83c7-4b59-82de-70108b5b3d27             day0  51.18\n",
       "4  2e9a0213-1a2a-4a40-88de-79e07becbcb7             day0  51.18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a helpful debug command: check a few rows for one product.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sample_product = 'p_16'\n",
    "print(f\"Product {sample_product} on day=0 had price {products.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "\n",
    "print(\"Prices in the combined field are all:\")\n",
    "normalized_view.filter(col('product') == sample_product) \\\n",
    "    .select('invoice_item_id', 'invoice_modified', 'price') \\\n",
    "    .orderBy(\"invoice_item_id\") \\\n",
    "    .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 50 products and deleting 0 products.\n",
      "Updating 582 invoices and deleting 4 invoices.\n",
      "Updating 1506 invoiceitems and deleting 19 invoiceitems.\n"
     ]
    }
   ],
   "source": [
    "# Day 1: process both updates and deletes, which come in separate files\n",
    "def read_data(table_location, day, has_deletes):\n",
    "    updates = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(f\"./data/{table_location}/updates/day={day}/\") \\\n",
    "                .drop('_c0')\n",
    "        \n",
    "    if has_deletes:\n",
    "        deletes = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(f\"./data/{table_location}/deletes/day={day}/\") \\\n",
    "                .drop(\"_c0\")\n",
    "    else:\n",
    "        deletes = None\n",
    "\n",
    "    return updates, deletes\n",
    "\n",
    "product_updates, _ = read_data(\"products\", day=1, has_deletes=False)\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\")\n",
    "print(f\"Updating {product_updates.count()} products and deleting 0 products.\")\n",
    "\n",
    "invoice_updates, invoice_deletes = read_data(\"invoice\", day=1, has_deletes=True)\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\")\n",
    "print(f\"Updating {invoice_updates.count()} invoices and deleting {invoice_deletes.count()} invoices.\")\n",
    "\n",
    "invoiceitem_updates, invoiceitem_deletes = read_data(\"invoiceitems\", day=1, has_deletes=True)\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\")\n",
    "print(f\"Updating {invoiceitem_updates.count()} invoiceitems and deleting {invoiceitem_deletes.count()} invoiceitems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 continued: merge tables\n",
    "from pyspark.sql.functions import lit\n",
    "product_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.product_id = newData.product_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "# build a set of update/deletes\n",
    "invoice_updates = invoice_updates.withColumn('operation', lit('update'))\n",
    "invoice_deletes = invoice_deletes.withColumn('operation', lit('delete'))\n",
    "invoices_full_updates = invoice_updates.unionAll(invoice_deletes)\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    invoices_full_updates.alias(\"newData\"),\n",
    "    \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "  .whenMatchedUpdateAll(condition='operation != \"delete\"') \\\n",
    "  .whenMatchedDelete(condition='operation = \"delete\"') \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "    \n",
    "invoiceitem_updates = invoiceitem_updates.withColumn('operation', lit('update'))\n",
    "invoiceitem_deletes = invoiceitem_deletes.withColumn('operation', lit('delete'))\n",
    "invoicesitem_full_updates = invoiceitem_updates.unionAll(invoiceitem_deletes)\n",
    "\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoicesitem_full_updates.alias(\"newData\"),\n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "  .whenMatchedUpdateAll(condition='operation != \"delete\"') \\\n",
    "  .whenMatchedDelete(condition='operation = \"delete\"') \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, every day we merge in a new set of data from a production system. This could Create, Update, or Delete rows in any table. (An example where deletes as opposed to soft deletes might happen is GDPR compliance.) So, every day we get updated Delta Lake tables representing each table. These are normally created with merge commands to take advantage of partitions.\n",
    "\n",
    "Every day we also need to rebuild our normalized table. This code is a copy of Day 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build normalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "normalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "normalized_view = normalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").mode(\"overwrite\").save(\"./outputs/normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things I hate about this join. First, we have to load the entire table every day to produce our join. If we tried to load, say, only data changed on `day=1` then we would risk join failures because of products that were not changed on day 1. For instance, say that a product is created on day 0 and used in an invoice on day 0. Mistimed data copies from the Products service and the Invoices service could result in the invoice copying over but the product not copying over. On day 1, the new product will be copied to the lake. If we rerun all data, the invoice from day 0 will be updated, but it means we have to load invoices from day 0 even if they didn't change!\n",
    "\n",
    "Second, the normalized data pulls the most recent value for any product not the value that was active when an invoice item was created. If we change the price in our product table, for instance, then the next day's normalized data will set that new price for all previous invoice items. This can be misleading!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product p_16 on day=0 had price 51.18\n",
      "Product p_16 on day=1 had price 13.72\n",
      "Prices in the combined field are all:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>invoice_item_modified</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01bd1f56-dfdc-47af-8062-bfc833d60b0b</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ec6f0c1-5efa-497b-9514-2fde40cd0077</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f69b5dd-b7e8-445a-8268-f317e4edc2bd</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11c4720e-a2b8-46e4-ba38-0be21b7a6931</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199dca1d-40b3-40e7-8ae0-df10b4d68e07</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1ca8b663-c6c4-4f3a-948b-e43ca25d2e17</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1dd96153-630b-447c-b438-9047586e25fe</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1f962845-105a-46c5-b835-0457eba2f996</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27f5a1f6-83c7-4b59-82de-70108b5b3d27</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2e9a0213-1a2a-4a40-88de-79e07becbcb7</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        invoice_item_id invoice_modified  \\\n",
       "0  01bd1f56-dfdc-47af-8062-bfc833d60b0b             day1   \n",
       "1  0ec6f0c1-5efa-497b-9514-2fde40cd0077             day1   \n",
       "2  0f69b5dd-b7e8-445a-8268-f317e4edc2bd             day0   \n",
       "3  11c4720e-a2b8-46e4-ba38-0be21b7a6931             day1   \n",
       "4  199dca1d-40b3-40e7-8ae0-df10b4d68e07             day1   \n",
       "5  1ca8b663-c6c4-4f3a-948b-e43ca25d2e17             day1   \n",
       "6  1dd96153-630b-447c-b438-9047586e25fe             day0   \n",
       "7  1f962845-105a-46c5-b835-0457eba2f996             day0   \n",
       "8  27f5a1f6-83c7-4b59-82de-70108b5b3d27             day0   \n",
       "9  2e9a0213-1a2a-4a40-88de-79e07becbcb7             day0   \n",
       "\n",
       "  invoice_item_modified  price  \n",
       "0                  day1  13.72  \n",
       "1                  day1  13.72  \n",
       "2                  day0  13.72  \n",
       "3                  day1  13.72  \n",
       "4                  day1  13.72  \n",
       "5                  day1  13.72  \n",
       "6                  day0  13.72  \n",
       "7                  day0  13.72  \n",
       "8                  day0  13.72  \n",
       "9                  day0  13.72  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "sample_product = 'p_16'\n",
    "print(f\"Product {sample_product} on day=0 had price {products.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "print(f\"Product {sample_product} on day=1 had price {product_updates.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "\n",
    "print(\"Prices in the combined field are all:\")\n",
    "normalized_view.filter(col('product') == sample_product).select('invoice_item_id', 'invoice_modified', 'invoice_item_modified', 'price').limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the invoice was created and only modified on day 0 when the price was 51.18, the price in our normalized view on day=1 was updated to read 13.72. This can be fixed, but requires more work.\n",
    "\n",
    "Enabling Change Tracking and Converting to Incremental Jobs\n",
    "---------------------------------------------------\n",
    "\n",
    "What we really want is to be able to track the changes that we introduce when day1 data merges into each of our three base tables `product`, `invoice`, and `invoice_item`. It turns out Delta Lake supports change tracking as of V2.0.0. They call this feature the [change data feed](https://docs.delta.io/2.0.0/delta-change-data-feed.html). We enabled it in the top cell of this notebook when we added this setting to our spark context:\n",
    "\n",
    "`.config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")`\n",
    "\n",
    "When you write data - Create, Update, or Delete - to a Delta table with Change Data Feed enabled, Delta lake writes additional parquet files that track which rows were inserted, created, or deleted in each transaction. You can read the change records by enabling option `.option('readChangeFeed', 'true')` during reads. Below we look at a few change records for the `invoice` and `product` tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "product_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change feed is a list of updates in the table between startingVersion and most recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>customer</th>\n",
       "      <th>status</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>invoice_created</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inv_134</td>\n",
       "      <td>a42c9381-3989-4660-ba88-fe4ceffa48ba</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>delete</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inv_587</td>\n",
       "      <td>6024b982-5c4e-4055-8488-3e9d1c7393ba</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>delete</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inv_0</td>\n",
       "      <td>22cb24c1-689f-4861-8614-f026649652c9</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-13 18:11:45.348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inv_1</td>\n",
       "      <td>61f0507e-37d3-4433-8368-e3ffc9f47a4a</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-13 18:11:45.348</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inv_1</td>\n",
       "      <td>61f0507e-37d3-4433-8368-e3ffc9f47a4a</td>\n",
       "      <td>sent</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inv_103</td>\n",
       "      <td>6f92d2f8-c5ae-409d-844c-e144698c4057</td>\n",
       "      <td>sent</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inv_1</td>\n",
       "      <td>61f0507e-37d3-4433-8368-e3ffc9f47a4a</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inv_103</td>\n",
       "      <td>6f92d2f8-c5ae-409d-844c-e144698c4057</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:28.195</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_id                              customer status invoice_modified  \\\n",
       "0    inv_134  a42c9381-3989-4660-ba88-fe4ceffa48ba   sent             day0   \n",
       "1    inv_587  6024b982-5c4e-4055-8488-3e9d1c7393ba   sent             day0   \n",
       "2      inv_0  22cb24c1-689f-4861-8614-f026649652c9   sent             day0   \n",
       "3      inv_1  61f0507e-37d3-4433-8368-e3ffc9f47a4a   sent             day0   \n",
       "4      inv_1  61f0507e-37d3-4433-8368-e3ffc9f47a4a   sent             day1   \n",
       "5    inv_103  6f92d2f8-c5ae-409d-844c-e144698c4057   sent             day1   \n",
       "6      inv_1  61f0507e-37d3-4433-8368-e3ffc9f47a4a   sent             day0   \n",
       "7    inv_103  6f92d2f8-c5ae-409d-844c-e144698c4057   sent             day0   \n",
       "\n",
       "  invoice_created      _change_type  _commit_version       _commit_timestamp  \\\n",
       "0            day0            delete                1 2022-11-13 18:12:28.195   \n",
       "1            day0            delete                1 2022-11-13 18:12:28.195   \n",
       "2            day0            insert                0 2022-11-13 18:11:45.348   \n",
       "3            day0            insert                0 2022-11-13 18:11:45.348   \n",
       "4            day0  update_postimage                1 2022-11-13 18:12:28.195   \n",
       "5            day0  update_postimage                1 2022-11-13 18:12:28.195   \n",
       "6            day0   update_preimage                1 2022-11-13 18:12:28.195   \n",
       "7            day0   update_preimage                1 2022-11-13 18:12:28.195   \n",
       "\n",
       "   sample  \n",
       "0       1  \n",
       "1       2  \n",
       "2       1  \n",
       "3       2  \n",
       "4       1  \n",
       "5       2  \n",
       "6       1  \n",
       "7       2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('invoice_id')\n",
    "invoice_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>product_modified</th>\n",
       "      <th>product_created</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p_0</td>\n",
       "      <td>widget0</td>\n",
       "      <td>15.66</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-13 18:11:32.094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p_1</td>\n",
       "      <td>sproket1</td>\n",
       "      <td>44.45</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-13 18:11:32.094</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p_103</td>\n",
       "      <td>sproket103</td>\n",
       "      <td>62.81</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:20.921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p_103</td>\n",
       "      <td>sproket103</td>\n",
       "      <td>8.45</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:20.921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p_106</td>\n",
       "      <td>sproket106</td>\n",
       "      <td>46.47</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:20.921</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p_106</td>\n",
       "      <td>sproket106</td>\n",
       "      <td>10.93</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-13 18:12:20.921</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id product_name  price product_modified product_created  \\\n",
       "0        p_0      widget0  15.66             day0            day0   \n",
       "1        p_1     sproket1  44.45             day0            day0   \n",
       "2      p_103   sproket103  62.81             day1            day0   \n",
       "3      p_103   sproket103   8.45             day0            day0   \n",
       "4      p_106   sproket106  46.47             day1            day0   \n",
       "5      p_106   sproket106  10.93             day0            day0   \n",
       "\n",
       "       _change_type  _commit_version       _commit_timestamp  sample  \n",
       "0            insert                0 2022-11-13 18:11:32.094       1  \n",
       "1            insert                0 2022-11-13 18:11:32.094       2  \n",
       "2  update_postimage                1 2022-11-13 18:12:20.921       1  \n",
       "3   update_preimage                1 2022-11-13 18:12:20.921       1  \n",
       "4  update_postimage                1 2022-11-13 18:12:20.921       2  \n",
       "5   update_preimage                1 2022-11-13 18:12:20.921       2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('product_id')\n",
    "product_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).orderBy('product_id').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_change_type` shows 4 different types:\n",
    "\n",
    "* insert is a create\n",
    "* delete is a delete\n",
    "* update_preimage is the before side of an update.\n",
    "* update_postimage is the after side of an update.\n",
    "\n",
    "Both update_preimage and update_postimage happen on day=1 (`_commit_version = 1`) and you can see the pre and post prices.\n",
    "\n",
    "The really cool thing about Change Data tracking is that we can create our normalized_view of data on day=1 without reading the entire existing normalized_view table. \n",
    "We accomplish this with a `merge` that uses Change Data input. To see this, I'm going to use time travel to get a copy of the `normalized_view` as it existed at day=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_view_day_0 = sc.read.format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", '0') \\\n",
    "  .load(\"./outputs/normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max(invoice_modified)</th>\n",
       "      <th>max(invoice_item_modified)</th>\n",
       "      <th>max(product_modified)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  max(invoice_modified) max(invoice_item_modified) max(product_modified)\n",
       "0                  day0                       day0                  day0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prove we have no day 1 data in our time travel:\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "normalized_view_day_0.agg(\n",
    "    F.max(col('invoice_modified')), \n",
    "    F.max(col('invoice_item_modified')), \n",
    "    F.max(col('product_modified'))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "invoice_change_data = invoice_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "invoice_items_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoiceitems\")\n",
    "\n",
    "invoice_items_change_data = invoice_items_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "products_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/products\")\n",
    "\n",
    "products_change_data = products_change_data.filter(col('_change_type') != 'update_preimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_view_day_0_copy = DeltaTable.forPath(sc, \"./outputs/normalized_copy\")\n",
    "\n",
    "normalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_items_change_data.alias('right'),\n",
    "        'left.invoice_item_id = right.invoice_item_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedDelete(condition = 'right._change_type = \"delete\"') \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        'invoice_item_id': 'right.invoice_item_id',\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    }).execute()\n",
    "\n",
    "normalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_change_data.alias('right'),\n",
    "        'left.invoice = right.invoice_id'           # NOTE THIS! We'll discuss below.\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice_id': 'right.invoice_id',\n",
    "        'customer': 'right.customer',\n",
    "        'invoice_modified': 'right.invoice_modified',\n",
    "        'invoice_created': 'right.invoice_created',\n",
    "        'status': 'right.status',\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedUpdate(                             # NOTE THIS! We'll discuss below.\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }).execute()                                # NOTE No whenNotMatched.\n",
    "    \n",
    "    \n",
    "final_merge = normalized_view_day_0_copy.alias('left').merge(\n",
    "        products_change_data.alias('right'),\n",
    "        'left.product = right.product_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'product_id': 'right.product_id',\n",
    "        'product_name': 'right.product_name',\n",
    "        'price': 'right.price',\n",
    "        'product_modified': 'right.product_modified',\n",
    "        'product_created': 'right.product_created'\n",
    "    }, condition ='right._change_type != \"delete\" and left.invoice_item_modified >= right.product_modified') \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merges above nearly reproduce the left joins that we used to create normalized_view. However, there are a few key differences.\n",
    "\n",
    "First, prices are only updated if the price is updated on a day that the invoice item is also modified. This logic should be improved. \n",
    "One could choose, for instance, to never update prices on invoices if the invoice item was already created and the price set. \n",
    "We can see below that invoices on days 0 and 1 have different prices in our new example. To reproduce the left join with it's price resets, you would drop the second condition in `right._change_type != \"delete\" and left.invoice_item_modified = right.product_modified`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>invoice_item_modified</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0f69b5dd-b7e8-445a-8268-f317e4edc2bd</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dd96153-630b-447c-b438-9047586e25fe</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01bd1f56-dfdc-47af-8062-bfc833d60b0b</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ec6f0c1-5efa-497b-9514-2fde40cd0077</td>\n",
       "      <td>day1</td>\n",
       "      <td>day1</td>\n",
       "      <td>13.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        invoice_item_id invoice_modified  \\\n",
       "0  0f69b5dd-b7e8-445a-8268-f317e4edc2bd             day0   \n",
       "1  1dd96153-630b-447c-b438-9047586e25fe             day0   \n",
       "2  01bd1f56-dfdc-47af-8062-bfc833d60b0b             day1   \n",
       "3  0ec6f0c1-5efa-497b-9514-2fde40cd0077             day1   \n",
       "\n",
       "  invoice_item_modified  price  \n",
       "0                  day0  51.18  \n",
       "1                  day0  51.18  \n",
       "2                  day1  13.72  \n",
       "3                  day1  13.72  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_window = Window.partitionBy(\"invoice_modified\").orderBy('invoice_item_id')\n",
    "normalized_view_day_0_copy.toDF() \\\n",
    "    .filter(col('product') == sample_product) \\\n",
    "    .withColumn('sample', row_number().over(sample_window)) \\\n",
    "    .filter(col('sample') < 3) \\\n",
    "    .select('invoice_item_id', 'invoice_modified', 'invoice_item_modified', 'price').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to pay special attention to:\n",
    "    \n",
    "1) We're using the same approach of left joining against InvoiceItem. This means that the merge condition should mimic the fields used \n",
    "in InvoiceItem left join Invoice. Do not accidently use \"invoice_id\" from Invoice as the left column. \n",
    "\n",
    "2) Be very careful about deletes in merge. We truly delete a row from normalized_view only if it is deleted from InvoiceItem - again this mimics\n",
    "the left join above. For Invoice and for Product, a \"delete\" merely resets values to blanks.  TODO! WHAT IS EMPTY?\n",
    "\n",
    "3) There is no whenNotMatched on invoices or products. Again, to mimic a left join we omit these. This post is intended to show how to mimic a left join. But the existing approach has a major race condition problem. If a product is written on day N, but an invoice using it doesn't appear until day N+1, the product will never be written. In a production system, you would need to deal with this somehow. For instance, you could always use the full products table rather than the product updates to control this race condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "=========\n",
    "\n",
    "Switching your Delta based pipeline to use Change Data feeds needs a little thought, but it can make a more efficient pipeline. One caveat: make sure you've updates to delta 2.0 or above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
