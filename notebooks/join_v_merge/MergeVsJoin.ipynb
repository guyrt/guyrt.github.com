{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to an Incremental Pipeline in Delta Lake: Change Tracking\n",
    "================================\n",
    "\n",
    "This post shows how to use Change Tracking (todo link) in Delta Lake 2.0 to convert a batch pipeline to an incremental update pipeline. We'll cover two parts:\n",
    "\n",
    "1. Capturing change tracking in a Delta Lake Merge job.\n",
    "1. Converting a series of `join` operations to `merge` operations to produce a cheaper pipeline using incremental operations.\n",
    "\n",
    "Setting Up a Scenario: 3 Tables\n",
    "--------------------------------------\n",
    "\n",
    "I've set up three tables:\n",
    "\n",
    "1. Invoice\n",
    "2. InvoiceItem\n",
    "3. Product\n",
    "\n",
    "The ground truth for these tables lives in a production system and is dumped to the data lake and merged into a delta lake table. The logic for this merge is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaChangeFeedExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")  \\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "    \n",
    "\n",
    "sc = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 0: Read the data and merge. This is just to get our tables set up. See Day 1 for a \"normal\" day.\n",
    "products = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/products/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "products.write.format(\"delta\").save(\"./outputs/products\")\n",
    "\n",
    "invoices = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoice/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoices.write.format(\"delta\").save(\"./outputs/invoices\")\n",
    "\n",
    "invoiceitems = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoiceitems/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoiceitems.write.format(\"delta\").save(\"./outputs/invoiceitems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New, assume there is a job that produces a denormalized copy of the data that merges all three tables together. This data has one row per invoice item. We can perform normalization using a couple of joins. Occasionally we see \"hiccups\" where an invoice and invoice item exist in our data lake but the product has not yet been downloaded. This kind of delay can happen when tables are joined that come from different production systems. So, we'll left join products because they will occasionally be null. Bad things happen in complicated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build denormalized view on day 0\n",
    "# build denormalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "denormalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "denormalized_view = denormalized_view.join(product_base, denormalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "denormalized_view.write.format(\"delta\").save(\"./outputs/denormalized\")\n",
    "\n",
    "denormalized_view.write.format(\"delta\").save(\"./outputs/denormalized_copy\")  # we'll use this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the price for these invoice items. We'll revisit them after day 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product p_16 on day=0 had price 51.18\n",
      "Prices in the combined field are all:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0f69b5dd-b7e8-445a-8268-f317e4edc2bd</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dd96153-630b-447c-b438-9047586e25fe</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f962845-105a-46c5-b835-0457eba2f996</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27f5a1f6-83c7-4b59-82de-70108b5b3d27</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2e9a0213-1a2a-4a40-88de-79e07becbcb7</td>\n",
       "      <td>day0</td>\n",
       "      <td>51.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        invoice_item_id invoice_modified  price\n",
       "0  0f69b5dd-b7e8-445a-8268-f317e4edc2bd             day0  51.18\n",
       "1  1dd96153-630b-447c-b438-9047586e25fe             day0  51.18\n",
       "2  1f962845-105a-46c5-b835-0457eba2f996             day0  51.18\n",
       "3  27f5a1f6-83c7-4b59-82de-70108b5b3d27             day0  51.18\n",
       "4  2e9a0213-1a2a-4a40-88de-79e07becbcb7             day0  51.18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a helpful debug command: check a few rows for one product.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sample_product = 'p_16'\n",
    "print(f\"Product {sample_product} on day=0 had price {products.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "\n",
    "print(\"Prices in the combined field are all:\")\n",
    "denormalized_view.filter(col('product') == sample_product) \\\n",
    "    .select('invoice_item_id', 'invoice_modified', 'price') \\\n",
    "    .orderBy(\"invoice_item_id\") \\\n",
    "    .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 50 products and deleting 0 products.\n",
      "Updating 582 invoices and deleting 4 invoices.\n",
      "Updating 1506 invoiceitems and deleting 19 invoiceitems.\n"
     ]
    }
   ],
   "source": [
    "# Day 1: process both updates and deletes, which come in separate files\n",
    "def read_data(table_location, day, has_deletes):\n",
    "    updates = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(f\"./data/{table_location}/updates/day={day}/\") \\\n",
    "                .drop('_c0')\n",
    "        \n",
    "    if has_deletes:\n",
    "        deletes = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(f\"./data/{table_location}/deletes/day={day}/\") \\\n",
    "                .drop(\"_c0\")\n",
    "    else:\n",
    "        deletes = None\n",
    "\n",
    "    return updates, deletes\n",
    "\n",
    "product_updates, _ = read_data(\"products\", day=1, has_deletes=False)\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\")\n",
    "print(f\"Updating {product_updates.count()} products and deleting 0 products.\")\n",
    "\n",
    "invoice_updates, invoice_deletes = read_data(\"invoice\", day=1, has_deletes=True)\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\")\n",
    "print(f\"Updating {invoice_updates.count()} invoices and deleting {invoice_deletes.count()} invoices.\")\n",
    "\n",
    "invoiceitem_updates, invoiceitem_deletes = read_data(\"invoiceitems\", day=1, has_deletes=True)\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\")\n",
    "print(f\"Updating {invoiceitem_updates.count()} invoiceitems and deleting {invoiceitem_deletes.count()} invoiceitems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 continued: merge tables\n",
    "from pyspark.sql.functions import lit\n",
    "product_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.product_id = newData.product_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "# build a set of update/deletes\n",
    "invoice_updates = invoice_updates.withColumn('operation', lit('update'))\n",
    "invoice_deletes = invoice_deletes.withColumn('operation', lit('delete'))\n",
    "invoices_full_updates = invoice_updates.unionAll(invoice_deletes)\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    invoices_full_updates.alias(\"newData\"),\n",
    "    \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "  .whenMatchedUpdateAll(condition='operation != \"delete\"') \\\n",
    "  .whenMatchedDelete(condition='operation = \"delete\"') \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "    \n",
    "invoiceitem_updates = invoiceitem_updates.withColumn('operation', lit('update'))\n",
    "invoiceitem_deletes = invoiceitem_deletes.withColumn('operation', lit('delete'))\n",
    "invoicesitem_full_updates = invoiceitem_updates.unionAll(invoiceitem_deletes)\n",
    "\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoicesitem_full_updates.alias(\"newData\"),\n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "  .whenMatchedUpdateAll(condition='operation != \"delete\"') \\\n",
    "  .whenMatchedDelete(condition='operation = \"delete\"') \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, every day we merge in a new set of data from a production system. This could Create, Update, or Delete rows in any table. (An example where deletes as opposed to soft deletes might happen is GDPR compliance.) So, every day we get updated Delta Lake tables representing each table. These are normally created with merge commands to take advantage of partitions.\n",
    "\n",
    "Every day we also need to rebuild our denormalized table. This code is a copy of Day 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-06f5cc2a94af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# not exist yet.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdenormalized_view\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvoiceitem_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvoice_base\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvoiceitem_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minvoice_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoice_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdenormalized_view\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdenormalized_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct_base\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproduct_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdenormalized_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./outputs/denormalized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_view' is not defined"
     ]
    }
   ],
   "source": [
    "# build denormalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "denormalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "denormalized_view = denormalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "denormalized_view.write.format(\"delta\").mode(\"overwrite\").save(\"./outputs/denormalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things I hate about this join. First, we have to load the entire table every day to produce our join. If we tried to load, say, only data changed on `day=1` then we would risk join failures because of products that were not changed on day 1. For instance, say that a product is created on day 0 and used in an invoice on day 0. Mistimed data copies from the Products service and the Invoices service could result in the invoice copying over but the product not copying over. On day 1, the new product will be copied to the lake. If we rerun all data, the invoice from day 0 will be updated, but it means we have to load invoices from day 0 even if they didn't change!\n",
    "\n",
    "Second, the denormalized data pulls the most recent value for any product not the value that was active when an invoice item was created. If we change the price in our product table, for instance, then the next day's denormalized data will set that new price for all previous invoice items. This can be misleading!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "sample_product = 'p_16'\n",
    "print(f\"Product {sample_product} on day=0 had price {products.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "print(f\"Product {sample_product} on day=1 had price {product_updates.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "\n",
    "print(\"Prices in the combined field are all:\")\n",
    "denormalized_view.filter(col('product') == sample_product).select('invoice_item_id', 'invoice_modified', 'invoice_item_modified', 'price').limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the invoice was created and only modified on day 0 when the price was 51.18, the price in our denormalized view on day=1 was updated to read 13.72. This can be fixed, but requires more work.\n",
    "\n",
    "Enabling Change Tracking and Converting to Incremental Jobs\n",
    "---------------------------------------------------\n",
    "\n",
    "What we really want is to be able to track the changes that we introduce when day1 data merges into each of our three base tables `product`, `invoice`, and `invoice_item`. It turns out Delta Lake supports change tracking as of V2.0.0. They call this feature the [change data feed](https://docs.delta.io/2.0.0/delta-change-data-feed.html). We enabled it in the top cell of this notebook when we added this setting to our spark context:\n",
    "\n",
    "`.config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")`\n",
    "\n",
    "When you write data - Create, Update, or Delete - to a Delta table with Change Data Feed enabled, Delta lake writes additional parquet files that track which rows were inserted, created, or deleted in each transaction. You can read the change records by enabling option `.option('readChangeFeed', 'true')` during reads. Below we look at a few change records for the `invoice` and `product` tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "product_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change feed is a list of updates in the table between startingVersion and most recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('invoice_id')\n",
    "invoice_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('product_id')\n",
    "product_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).orderBy('product_id').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_change_type` shows 4 different types:\n",
    "\n",
    "* insert is a create\n",
    "* delete is a delete\n",
    "* update_preimage is the before side of an update.\n",
    "* update_postimage is the after side of an update.\n",
    "\n",
    "Both update_preimage and update_postimage happen on day=1 (`_commit_version = 1`) and you can see the pre and post prices.\n",
    "\n",
    "The really cool thing about Change Data tracking is that we can create our denormalized_view of data on day=1 without reading the entire existing denormalized_view table. \n",
    "We accomplish this with a `merge` that uses Change Data input. To see this, I'm going to use time travel to get a copy of the `normalized_view` as it existed at day=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_view_day_0 = sc.read.format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", '0') \\\n",
    "  .load(\"./outputs/denormalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prove we have no day 1 data in our time travel:\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "denormalized_view_day_0.agg(\n",
    "    F.max(col('invoice_modified')), \n",
    "    F.max(col('invoice_item_modified')), \n",
    "    F.max(col('product_modified'))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "invoice_change_data = invoice_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "invoice_items_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoiceitems\")\n",
    "\n",
    "invoice_items_change_data = invoice_items_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "products_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/products\")\n",
    "\n",
    "products_change_data = products_change_data.filter(col('_change_type') != 'update_preimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_view_day_0_copy = DeltaTable.forPath(sc, \"./outputs/denormalized_copy\")\n",
    "\n",
    "denormalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_items_change_data.alias('right'),\n",
    "        'left.invoice_item_id = right.invoice_item_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedDelete(condition = 'right._change_type = \"delete\"') \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        'invoice_item_id': 'right.invoice_item_id',\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    }).execute()\n",
    "\n",
    "denormalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_change_data.alias('right'),\n",
    "        'left.invoice = right.invoice_id'           # NOTE THIS! We'll discuss below.\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice_id': 'right.invoice_id',\n",
    "        'customer': 'right.customer',\n",
    "        'invoice_modified': 'right.invoice_modified',\n",
    "        'invoice_created': 'right.invoice_created',\n",
    "        'status': 'right.status',\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedUpdate(                             # NOTE THIS! We'll discuss below.\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }).execute()                                # NOTE No whenNotMatched.\n",
    "    \n",
    "    \n",
    "final_merge = denormalized_view_day_0_copy.alias('left').merge(\n",
    "        products_change_data.alias('right'),\n",
    "        'left.product = right.product_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'product_id': 'right.product_id',\n",
    "        'product_name': 'right.product_name',\n",
    "        'price': 'right.price',\n",
    "        'product_modified': 'right.product_modified',\n",
    "        'product_created': 'right.product_created'\n",
    "    }, condition ='right._change_type != \"delete\" and left.invoice_item_modified >= right.product_modified') \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merges above nearly reproduce the left joins that we used to create denormalized_view. However, there are a few key differences.\n",
    "\n",
    "First, prices are only updated if the price is updated on a day that the invoice item is also modified. This logic should be improved. \n",
    "One could choose, for instance, to never update prices on invoices if the invoice item was already created and the price set. \n",
    "We can see below that invoices on days 0 and 1 have different prices in our new example. To reproduce the left join with it's price resets, you would drop the second condition in `right._change_type != \"delete\" and left.invoice_item_modified = right.product_modified`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_window = Window.partitionBy(\"invoice_modified\").orderBy('invoice_item_id')\n",
    "denormalized_view_day_0_copy.toDF() \\\n",
    "    .filter(col('product') == sample_product) \\\n",
    "    .withColumn('sample', row_number().over(sample_window)) \\\n",
    "    .filter(col('sample') < 3) \\\n",
    "    .select('invoice_item_id', 'invoice_modified', 'invoice_item_modified', 'price').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to pay special attention to:\n",
    "    \n",
    "1) We're using the same approach of left joining against InvoiceItem. This means that the merge condition should mimic the fields used \n",
    "in InvoiceItem left join Invoice. Do not accidently use \"invoice_id\" from Invoice as the left column. \n",
    "\n",
    "2) Be very careful about deletes in merge. We truly delete a row from denormalized_view only if it is deleted from InvoiceItem - again this mimics\n",
    "the left join above. For Invoice and for Product, a \"delete\" merely resets values to blanks.  TODO! WHAT IS EMPTY?\n",
    "\n",
    "3) There is no whenNotMatched on invoices or products. Again, to mimic a left join we omit these. This post is intended to show how to mimic a left join. But the existing approach has a major race condition problem. If a product is written on day N, but an invoice using it doesn't appear until day N+1, the product will never be written. In a production system, you would need to deal with this somehow. For instance, you could always use the full products table rather than the product updates to control this race condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "=========\n",
    "\n",
    "Switching your Delta based pipeline to use Change Data feeds needs a little thought, but it can make a more efficient pipeline. One caveat: make sure you've updates to delta 2.0 or above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
