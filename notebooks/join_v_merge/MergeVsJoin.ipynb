{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to an Incremental Pipeline in Delta Lake: Change Tracking\n",
    "================================\n",
    "\n",
    "This post shows how to use Change Tracking (todo link) in Delta Lake 2.0 to convert a batch pipeline to an incremental update pipeline. We'll cover two parts:\n",
    "\n",
    "1. Capturing change tracking in a Delta Lake Merge job.\n",
    "1. Converting a series of `join` operations to `merge` operations to produce a cheaper pipeline using incremental operations.\n",
    "\n",
    "Setting Up a Scenario: 3 Tables\n",
    "--------------------------------------\n",
    "\n",
    "I've set up three tables:\n",
    "\n",
    "1. Invoice\n",
    "2. InvoiceItem\n",
    "3. Product\n",
    "\n",
    "The ground truth for these tables lives in a production system and is dumped to the data lake and merged into a delta lake table. The logic for this merge is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "\n",
    "sc = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 0: Read the data and merge. This is just to get our tables set up. See Day 1 for a \"normal\" day.\n",
    "products = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/products/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "products.write.format(\"delta\").save(\"./outputs/products\")\n",
    "\n",
    "invoices = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoices/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoices.write.format(\"delta\").save(\"./outputs/invoices\")\n",
    "\n",
    "invoiceitems = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoiceitems/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoiceitems.write.format(\"delta\").save(\"./outputs/invoiceitems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 50 products and deleting 0 products.\n",
      "Updating 444 invoices and deleting 3 invoices.\n",
      "Updating 1533 invoiceitems and deleting 8 invoiceitems.\n"
     ]
    }
   ],
   "source": [
    "# Day 1: process both updates and deletes, which come in separate files\n",
    "def read_data(table_location, day, has_deletes):\n",
    "    updates = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(f\"./data/{table_location}/updates/day={day}/\") \\\n",
    "                .drop('_c0')\n",
    "        \n",
    "    if has_deletes:\n",
    "        deletes = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(f\"./data/{table_location}/deletes/day={day}/\") \\\n",
    "                .drop(\"_c0\")\n",
    "    else:\n",
    "        deletes = None\n",
    "\n",
    "    return updates, deletes\n",
    "\n",
    "product_updates, _ = read_data(\"products\", day=1, has_deletes=False)\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\")\n",
    "print(f\"Updating {product_updates.count()} products and deleting 0 products.\")\n",
    "\n",
    "invoice_updates, invoice_deletes = read_data(\"invoice\", day=1, has_deletes=True)\n",
    "print(f\"Updating {invoice_updates.count()} invoices and deleting {invoice_deletes.count()} invoices.\")\n",
    "\n",
    "invoiceitem_updates, invoiceitem_deletes = read_data(\"invoiceitems\", day=1, has_deletes=True)\n",
    "print(f\"Updating {invoiceitem_updates.count()} invoiceitems and deleting {invoiceitem_deletes.count()} invoiceitems.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, every day we merge in a new set of data from a production system. This could Create, Update, or Delete rows in any table. (Normally I would advise against delete in these cases, but GDPR or other constraints may mean we have deletes in addition to updates and creates). So, every day we get updated Delta Lake tables representing each table.\n",
    "\n",
    "\n",
    "We also have a job that produces a normalized copy of the data that merges all three tables together. This data has one row per invoice item. We can perform normalization using a couple of joins. Occasionally we see \"hiccups\" where an invoice and invoice item exist in our data lake but the product has not yet been downloaded. This kind of delay can happen when tables are joined that come from different production systems. So, we'll left join products because they will occasionally be null. Bad things happen in complicated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build normalized join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble with this:\n",
    "* It's expensive and repetitive\n",
    "* Product changes get reflected in historical invoices without more care. For instance, we switch the quantity of widgets from 6 to 4 per package without changing the SKU. This is very bad don't ever change the meaning of a SKU. Sigh.\n",
    "\n",
    "Enabling Change Tracking and Converting to Incremental Jobs\n",
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new spark context with stuff enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
