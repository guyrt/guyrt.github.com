{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to an Incremental Pipeline in Delta Lake: Change Tracking\n",
    "================================\n",
    "\n",
    "This post shows how to use Change Tracking (todo link) in Delta Lake 2.0 to convert a batch pipeline to an incremental update pipeline. We'll cover two parts:\n",
    "\n",
    "1. Capturing change tracking in a Delta Lake Merge job.\n",
    "1. Converting a series of `join` operations to `merge` operations to produce a cheaper pipeline using incremental operations.\n",
    "\n",
    "Setting Up a Scenario: 3 Tables\n",
    "--------------------------------------\n",
    "\n",
    "I've set up three tables:\n",
    "\n",
    "1. Invoice\n",
    "2. InvoiceItem\n",
    "3. Product\n",
    "\n",
    "The ground truth for these tables lives in a production system and is dumped to the data lake and merged into a delta lake table. The logic for this merge is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "\n",
    "sc = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 0: Read the data and merge. This is just to get our tables set up. See Day 1 for a \"normal\" day.\n",
    "products = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/products/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "products.write.format(\"delta\").save(\"./outputs/products\")\n",
    "\n",
    "invoices = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoice/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoices.write.format(\"delta\").save(\"./outputs/invoices\")\n",
    "\n",
    "invoiceitems = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoiceitems/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoiceitems.write.format(\"delta\").save(\"./outputs/invoiceitems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 50 products and deleting 0 products.\n",
      "Updating 444 invoices and deleting 3 invoices.\n",
      "Updating 1533 invoiceitems and deleting 8 invoiceitems.\n"
     ]
    }
   ],
   "source": [
    "# Day 1: process both updates and deletes, which come in separate files\n",
    "def read_data(table_location, day, has_deletes):\n",
    "    updates = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(f\"./data/{table_location}/updates/day={day}/\") \\\n",
    "                .drop('_c0')\n",
    "        \n",
    "    if has_deletes:\n",
    "        deletes = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(f\"./data/{table_location}/deletes/day={day}/\") \\\n",
    "                .drop(\"_c0\")\n",
    "    else:\n",
    "        deletes = None\n",
    "\n",
    "    return updates, deletes\n",
    "\n",
    "product_updates, _ = read_data(\"products\", day=1, has_deletes=False)\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\")\n",
    "print(f\"Updating {product_updates.count()} products and deleting 0 products.\")\n",
    "\n",
    "invoice_updates, invoice_deletes = read_data(\"invoice\", day=1, has_deletes=True)\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\")\n",
    "print(f\"Updating {invoice_updates.count()} invoices and deleting {invoice_deletes.count()} invoices.\")\n",
    "\n",
    "invoiceitem_updates, invoiceitem_deletes = read_data(\"invoiceitems\", day=1, has_deletes=True)\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\")\n",
    "print(f\"Updating {invoiceitem_updates.count()} invoiceitems and deleting {invoiceitem_deletes.count()} invoiceitems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 continued: merge tables\n",
    "product_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.product_id = newData.product_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "    .merge(invoice_deletes.alias(\"newData\"), \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "    .whenMatchedDelete() \\\n",
    "    .execute()\n",
    "\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoiceitem_updates.alias(\"newData\"),\n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll()\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoiceitem_deletes.alias(\"newData\"),         \n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "    .whenMatchedDelete() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, every day we merge in a new set of data from a production system. This could Create, Update, or Delete rows in any table. (An example where deletes as opposed to soft deletes might happen is GDPR compliance.) So, every day we get updated Delta Lake tables representing each table. These are normally created with merge commands to take advantage of partitions.\n",
    "\n",
    "\n",
    "New, assume there is a job that produces a normalized copy of the data that merges all three tables together. This data has one row per invoice item. We can perform normalization using a couple of joins. Occasionally we see \"hiccups\" where an invoice and invoice item exist in our data lake but the product has not yet been downloaded. This kind of delay can happen when tables are joined that come from different production systems. So, we'll left join products because they will occasionally be null. Bad things happen in complicated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build normalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "normalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice_item_id == invoice_base.invoice_id, how=\"left\")\n",
    "normalized_view = normalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").save(\"./outputs/normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things I hate about this join. First, we have to load the entire table every day to produce our join. If we tried to load, say, only data changed on day=1 then we would risk join failures because of products that were not changed on day 1.\n",
    "\n",
    "Second, the normalized data pulls the most recent value for any product not the value that was active when an invoice item was created. If we change the price in our product table, for instance, then the next day's normalized data will set that new price for all previous invoice items. This can be misleading!\n",
    "\n",
    "\n",
    "Enabling Change Tracking and Converting to Incremental Jobs\n",
    "---------------------------------------------------\n",
    "\n",
    "Say delta 2.0 fixes this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new spark context with stuff enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
