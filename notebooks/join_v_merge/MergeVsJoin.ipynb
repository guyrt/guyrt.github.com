{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to an Incremental Pipeline in Delta Lake: Change Tracking\n",
    "================================\n",
    "\n",
    "This post shows how to use Change Tracking (todo link) in Delta Lake 2.0 to convert a batch pipeline to an incremental update pipeline. We'll cover two parts:\n",
    "\n",
    "1. Capturing change tracking in a Delta Lake Merge job.\n",
    "1. Converting a series of `join` operations to `merge` operations to produce a cheaper pipeline using incremental operations.\n",
    "\n",
    "Setting Up a Scenario: 3 Tables\n",
    "--------------------------------------\n",
    "\n",
    "I've set up three tables:\n",
    "\n",
    "1. Invoice\n",
    "2. InvoiceItem\n",
    "3. Product\n",
    "\n",
    "The ground truth for these tables lives in a production system and is dumped to the data lake and merged into a delta lake table. The logic for this merge is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaChangeFeedExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")  \\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "    \n",
    "\n",
    "sc = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 0: Read the data and merge. This is just to get our tables set up. See Day 1 for a \"normal\" day.\n",
    "products = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/products/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "products.write.format(\"delta\").save(\"./outputs/products\")\n",
    "\n",
    "invoices = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoice/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoices.write.format(\"delta\").save(\"./outputs/invoices\")\n",
    "\n",
    "invoiceitems = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(\"./data/invoiceitems/updates/day=0/\") \\\n",
    "                .drop('_c0')\n",
    "\n",
    "invoiceitems.write.format(\"delta\").save(\"./outputs/invoiceitems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New, assume there is a job that produces a normalized copy of the data that merges all three tables together. This data has one row per invoice item. We can perform normalization using a couple of joins. Occasionally we see \"hiccups\" where an invoice and invoice item exist in our data lake but the product has not yet been downloaded. This kind of delay can happen when tables are joined that come from different production systems. So, we'll left join products because they will occasionally be null. Bad things happen in complicated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build normalized view on day 0\n",
    "# build normalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "normalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "normalized_view = normalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").save(\"./outputs/normalized\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").save(\"./outputs/normalized_copy\")  # we'll use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 50 products and deleting 0 products.\n",
      "Updating 448 invoices and deleting 5 invoices.\n",
      "Updating 1475 invoiceitems and deleting 18 invoiceitems.\n"
     ]
    }
   ],
   "source": [
    "# Day 1: process both updates and deletes, which come in separate files\n",
    "def read_data(table_location, day, has_deletes):\n",
    "    updates = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .load(f\"./data/{table_location}/updates/day={day}/\") \\\n",
    "                .drop('_c0')\n",
    "        \n",
    "    if has_deletes:\n",
    "        deletes = sc.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(f\"./data/{table_location}/deletes/day={day}/\") \\\n",
    "                .drop(\"_c0\")\n",
    "    else:\n",
    "        deletes = None\n",
    "\n",
    "    return updates, deletes\n",
    "\n",
    "product_updates, _ = read_data(\"products\", day=1, has_deletes=False)\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\")\n",
    "print(f\"Updating {product_updates.count()} products and deleting 0 products.\")\n",
    "\n",
    "invoice_updates, invoice_deletes = read_data(\"invoice\", day=1, has_deletes=True)\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\")\n",
    "print(f\"Updating {invoice_updates.count()} invoices and deleting {invoice_deletes.count()} invoices.\")\n",
    "\n",
    "invoiceitem_updates, invoiceitem_deletes = read_data(\"invoiceitems\", day=1, has_deletes=True)\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\")\n",
    "print(f\"Updating {invoiceitem_updates.count()} invoiceitems and deleting {invoiceitem_deletes.count()} invoiceitems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 continued: merge tables\n",
    "product_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.product_id = newData.product_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "  .merge(\n",
    "    product_updates.alias(\"newData\"),\n",
    "    \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "\n",
    "invoice_base.alias(\"oldData\") \\\n",
    "    .merge(invoice_deletes.alias(\"newData\"), \"oldData.invoice_id = newData.invoice_id\") \\\n",
    "    .whenMatchedDelete() \\\n",
    "    .execute()\n",
    "\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoiceitem_updates.alias(\"newData\"),\n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll()\n",
    "invoiceitem_base.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "        invoiceitem_deletes.alias(\"newData\"),         \n",
    "        \"oldData.invoice_item_id = newData.invoice_item_id\"\n",
    "    ) \\\n",
    "    .whenMatchedDelete() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, every day we merge in a new set of data from a production system. This could Create, Update, or Delete rows in any table. (An example where deletes as opposed to soft deletes might happen is GDPR compliance.) So, every day we get updated Delta Lake tables representing each table. These are normally created with merge commands to take advantage of partitions.\n",
    "\n",
    "Every day we also need to rebuild our normalized table. This code is a copy of Day 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build normalized join\n",
    "product_base = DeltaTable.forPath(sc, \"./outputs/products\").toDF()\n",
    "invoice_base = DeltaTable.forPath(sc, \"./outputs/invoices\").toDF()\n",
    "invoiceitem_base = DeltaTable.forPath(sc, \"./outputs/invoiceitems\").toDF()\n",
    "\n",
    "# Left join with invoice item as the root. This isn't important for invoices and invoice items, but is\n",
    "# critical for products in this example since products may be pulled at different time cadence and, thus,\n",
    "# not exist yet.\n",
    "normalized_view = invoiceitem_base.join(invoice_base, invoiceitem_base.invoice == invoice_base.invoice_id, how=\"left\")\n",
    "normalized_view = normalized_view.join(product_base, normalized_view.product == product_base.product_id, how=\"left\")\n",
    "\n",
    "normalized_view.write.format(\"delta\").mode(\"overwrite\").save(\"./outputs/normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things I hate about this join. First, we have to load the entire table every day to produce our join. If we tried to load, say, only data changed on `day=1` then we would risk join failures because of products that were not changed on day 1. For instance, say that a product is created on day 0 and used in an invoice on day 0. Mistimed data copies from the Products service and the Invoices service could result in the invoice copying over but the product not copying over. On day 1, the new product will be copied to the lake. If we rerun all data, the invoice from day 0 will be updated, but it means we have to load invoices from day 0 even if they didn't change!\n",
    "\n",
    "Second, the normalized data pulls the most recent value for any product not the value that was active when an invoice item was created. If we change the price in our product table, for instance, then the next day's normalized data will set that new price for all previous invoice items. This can be misleading!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product p_0 on day=0 had price 25.36\n",
      "Product p_0 on day=1 had price 54.95\n",
      "Prices in the combined field are all:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bcf91a16-abf2-4504-a7c8-9e91342c543f</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19974cd5-9cef-4562-bd58-535e7f80db49</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2e04c9a3-a2e9-4c06-a872-52199eff51f1</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>668e762d-a4f8-4d6d-9cfc-d381744d1d4d</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cb5f59db-c24a-4d0b-81c2-de7f50646985</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21d11808-bd5b-4145-84c5-1da0bade6988</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e9a1c8-a27d-45c9-9431-4fb18e084bfa</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>387c97c7-f69f-4d3b-b34c-6de473649d33</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9997d809-8eee-49eb-bc61-d74ff0ff8abf</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f2ba58a7-afb1-461e-b1e7-3fb0f504b964</td>\n",
       "      <td>day0</td>\n",
       "      <td>54.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        invoice_item_id invoice_modified  price\n",
       "0  bcf91a16-abf2-4504-a7c8-9e91342c543f             day0  54.95\n",
       "1  19974cd5-9cef-4562-bd58-535e7f80db49             day0  54.95\n",
       "2  2e04c9a3-a2e9-4c06-a872-52199eff51f1             day0  54.95\n",
       "3  668e762d-a4f8-4d6d-9cfc-d381744d1d4d             day0  54.95\n",
       "4  cb5f59db-c24a-4d0b-81c2-de7f50646985             day0  54.95\n",
       "5  21d11808-bd5b-4145-84c5-1da0bade6988             day0  54.95\n",
       "6  29e9a1c8-a27d-45c9-9431-4fb18e084bfa             day0  54.95\n",
       "7  387c97c7-f69f-4d3b-b34c-6de473649d33             day0  54.95\n",
       "8  9997d809-8eee-49eb-bc61-d74ff0ff8abf             day0  54.95\n",
       "9  f2ba58a7-afb1-461e-b1e7-3fb0f504b964             day0  54.95"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "sample_product = 'p_0'\n",
    "print(f\"Product {sample_product} on day=0 had price {products.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "print(f\"Product {sample_product} on day=1 had price {product_updates.filter(col('product_id') == sample_product).collect()[0].asDict()['price']}\")\n",
    "\n",
    "print(\"Prices in the combined field are all:\")\n",
    "normalized_view.filter(col('product') == sample_product).select('invoice_item_id', 'invoice_modified', 'price').limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the invoice was created and only modified on day 0 when the price was 43.66, the price in our normalized view on day=1 was updated to read 54.95. This can be fixed, but requires more work.\n",
    "\n",
    "Enabling Change Tracking and Converting to Incremental Jobs\n",
    "---------------------------------------------------\n",
    "\n",
    "What we really want is to be able to track the changes that we introduce when day1 data merges into each of our three base tables `product`, `invoice`, and `invoice_item`. It turns out Delta Lake supports change tracking as of V2.0.0. They call this feature the [change data feed](https://docs.delta.io/2.0.0/delta-change-data-feed.html). We enabled it in the top cell of this notebook when we added this setting to our spark context:\n",
    "\n",
    "`.config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")`\n",
    "\n",
    "When you write data - Create, Update, or Delete - to a Delta table with Change Data Feed enabled, Delta lake writes additional parquet files that track which rows were inserted, created, or deleted in each transaction. You can read the change records by enabling option `.option('readChangeFeed', 'true')` during reads. Below we look at a few change records for the `invoice` and `product` tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "product_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '0') \\\n",
    "  .load(\"./outputs/products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change feed is a list of updates in the table between startingVersion and most recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>customer</th>\n",
       "      <th>status</th>\n",
       "      <th>invoice_modified</th>\n",
       "      <th>invoice_created</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inv_313</td>\n",
       "      <td>8a924220-24d8-4fc5-9d43-a1c20a769ea9</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>delete</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:12.378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inv_334</td>\n",
       "      <td>f9e4bd75-5cfe-48de-a6b2-b25b66eef8b0</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>delete</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:12.378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inv_0</td>\n",
       "      <td>70a17298-62dd-4eb8-aeb0-5ac32e2add65</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-12 14:29:55.968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inv_1</td>\n",
       "      <td>e84f85dc-68c4-42e2-84d5-74288f60a2a0</td>\n",
       "      <td>sent</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-12 14:29:55.968</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_id                              customer status invoice_modified  \\\n",
       "0    inv_313  8a924220-24d8-4fc5-9d43-a1c20a769ea9   sent             day0   \n",
       "1    inv_334  f9e4bd75-5cfe-48de-a6b2-b25b66eef8b0   sent             day0   \n",
       "2      inv_0  70a17298-62dd-4eb8-aeb0-5ac32e2add65   sent             day0   \n",
       "3      inv_1  e84f85dc-68c4-42e2-84d5-74288f60a2a0   sent             day0   \n",
       "\n",
       "  invoice_created _change_type  _commit_version       _commit_timestamp  \\\n",
       "0            day0       delete                1 2022-11-12 14:30:12.378   \n",
       "1            day0       delete                1 2022-11-12 14:30:12.378   \n",
       "2            day0       insert                0 2022-11-12 14:29:55.968   \n",
       "3            day0       insert                0 2022-11-12 14:29:55.968   \n",
       "\n",
       "   sample  \n",
       "0       1  \n",
       "1       2  \n",
       "2       1  \n",
       "3       2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('invoice_id')\n",
    "invoice_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>product_modified</th>\n",
       "      <th>product_created</th>\n",
       "      <th>_change_type</th>\n",
       "      <th>_commit_version</th>\n",
       "      <th>_commit_timestamp</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p_0</td>\n",
       "      <td>widget0</td>\n",
       "      <td>25.36</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-12 14:29:49.338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p_0</td>\n",
       "      <td>widget0</td>\n",
       "      <td>54.95</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:09.294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p_0</td>\n",
       "      <td>widget0</td>\n",
       "      <td>25.36</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:09.294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p_1</td>\n",
       "      <td>sproket1</td>\n",
       "      <td>65.8</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>insert</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-12 14:29:49.338</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p_10</td>\n",
       "      <td>sproket10</td>\n",
       "      <td>28.37</td>\n",
       "      <td>day1</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_postimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:09.294</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p_10</td>\n",
       "      <td>sproket10</td>\n",
       "      <td>57.55</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>update_preimage</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-12 14:30:09.294</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id product_name  price product_modified product_created  \\\n",
       "0        p_0      widget0  25.36             day0            day0   \n",
       "1        p_0      widget0  54.95             day1            day0   \n",
       "2        p_0      widget0  25.36             day0            day0   \n",
       "3        p_1     sproket1   65.8             day0            day0   \n",
       "4       p_10    sproket10  28.37             day1            day0   \n",
       "5       p_10    sproket10  57.55             day0            day0   \n",
       "\n",
       "       _change_type  _commit_version       _commit_timestamp  sample  \n",
       "0            insert                0 2022-11-12 14:29:49.338       1  \n",
       "1  update_postimage                1 2022-11-12 14:30:09.294       1  \n",
       "2   update_preimage                1 2022-11-12 14:30:09.294       1  \n",
       "3            insert                0 2022-11-12 14:29:49.338       2  \n",
       "4  update_postimage                1 2022-11-12 14:30:09.294       2  \n",
       "5   update_preimage                1 2022-11-12 14:30:09.294       2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_window = Window.partitionBy(\"_change_type\").orderBy('product_id')\n",
    "product_change_data.withColumn('sample', row_number().over(sample_window)).filter(col('sample') < 3).orderBy('product_id').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_change_type` shows 4 different types:\n",
    "\n",
    "* insert is a create\n",
    "* delete is a delete\n",
    "* update_preimage is the before side of an update.\n",
    "* update_postimage is the after side of an update.\n",
    "\n",
    "Both update_preimage and update_postimage happen on day=1 (`_commit_version = 1`) and you can see the pre and post prices.\n",
    "\n",
    "The really cool thing about Change Data tracking is that we can create our normalized_view of data on day=1 without reading the entire existing normalized_view table. \n",
    "We accomplish this with a `merge` that uses Change Data input. To see this, I'm going to use time travel to get a copy of the `normalized_view` as it existed at day=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_view_day_0 = sc.read.format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", '0') \\\n",
    "  .load(\"./outputs/normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max(invoice_modified)</th>\n",
       "      <th>max(invoice_item_modified)</th>\n",
       "      <th>max(product_modified)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "      <td>day0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  max(invoice_modified) max(invoice_item_modified) max(product_modified)\n",
       "0                  day0                       day0                  day0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prove we have no day 1 data in our time travel:\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "normalized_view_day_0.agg(\n",
    "    F.max(col('invoice_modified')), \n",
    "    F.max(col('invoice_item_modified')), \n",
    "    F.max(col('product_modified'))).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoices\")\n",
    "\n",
    "invoice_change_data = invoice_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "invoice_items_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/invoiceitems\")\n",
    "\n",
    "invoice_items_change_data = invoice_items_change_data.filter(col('_change_type') != 'update_preimage')\n",
    "\n",
    "products_change_data = sc.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", '1') \\\n",
    "  .load(\"./outputs/products\")\n",
    "\n",
    "products_change_data = products_change_data.filter(col('_change_type') != 'update_preimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_view_day_0_copy = DeltaTable.forPath(sc, \"./outputs/normalized_copy\")\n",
    "\n",
    "normalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_items_change_data.alias('right'),\n",
    "        'left.invoice_item_id = right.invoice_item_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedDelete(condition = 'right._change_type = \"delete\"') \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        'invoice_item_id': 'right.invoice_item_id',\n",
    "        'invoice': 'right.invoice',\n",
    "        'count': 'right.count',\n",
    "        'invoice_item_modified': 'right.invoice_item_modified',\n",
    "        'invoice_item_created': 'right.invoice_item_created',\n",
    "        'product': 'right.product'\n",
    "    })\n",
    "\n",
    "normalized_view_day_0_copy.alias('left').merge(\n",
    "        invoice_change_data.alias('right'),\n",
    "        'left.invoice = right.invoice_id'           # NOTE THIS! We'll discuss below.\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'invoice_id': 'right.invoice_id',\n",
    "        'customer': 'right.customer',\n",
    "        'invoice_modified': 'right.invoice_modified',\n",
    "        'invoice_created': 'right.invoice_created',\n",
    "        'status': 'right.status',\n",
    "    }, condition = 'right._change_type != \"delete\"') \\\n",
    "    .whenMatchedUpdate(                             # NOTE THIS! We'll discuss below.\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(values={                     # Note THIS! We'll discuss below.\n",
    "        'invoice_id': 'right.invoice_id',\n",
    "        'customer': 'right.customer',\n",
    "        'invoice_modified': 'right.invoice_modified',\n",
    "        'invoice_created': 'right.invoice_created',\n",
    "        'status': 'right.status'\n",
    "    })\n",
    "    \n",
    "    \n",
    "final_merge = normalized_view_day_0_copy.alias('left').merge(\n",
    "        products_change_data.alias('right'),\n",
    "        'left.product = right.product_id'\n",
    "    ).whenMatchedUpdate(set={\n",
    "        'product_id': 'right.product_id',\n",
    "        'product_name': 'right.product_name',\n",
    "        'price': 'right.price',\n",
    "        'product_modified': 'right.product_modified',\n",
    "        'product_created': 'right.product_created'\n",
    "    }, condition ='right._change_type != \"delete\"') \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition ='right._change_type = \"delete\"',\n",
    "        set={\n",
    "            'invoice_id': 'NULL',\n",
    "            'invoice': 'NULL',\n",
    "            'customer': 'NULL',\n",
    "            'invoice_modified': 'NULL',\n",
    "            'invoice_created': 'NULL',\n",
    "            'status': 'NULL'\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        'product_id': 'right.product_id',\n",
    "        'product_name': 'right.product_name',\n",
    "        'price': 'right.price',\n",
    "        'product_modified': 'right.product_modified',\n",
    "        'product_created': 'right.product_created'\n",
    "    })\n",
    "    \n",
    "final_merge.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# talk about each of your issues.\n",
    "# check the outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
