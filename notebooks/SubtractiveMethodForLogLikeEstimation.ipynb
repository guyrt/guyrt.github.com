{
 "metadata": {
  "name": "",
  "signature": "sha256:f9793bcea513d95478a0f01f0792ee6cff4ea22ec6165b0c9d5b74556beccca6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hey Mike,\n",
      "\n",
      "See below as a followup to our email. I would be interested in your thoughts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "import patsy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 292
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use a small dataset with correlations for this example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = sm.datasets.anes96.data.load_pandas().data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data.columns\n",
      "print data.dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'popul', u'TVnews', u'selfLR', u'ClinLR', u'DoleLR', u'PID', u'age', u'educ', u'income', u'vote', u'logpopul'], dtype='object')\n",
        "popul       float64\n",
        "TVnews      float64\n",
        "selfLR      float64\n",
        "ClinLR      float64\n",
        "DoleLR      float64\n",
        "PID         float64\n",
        "age         float64\n",
        "educ        float64\n",
        "income      float64\n",
        "vote        float64\n",
        "logpopul    float64\n",
        "dtype: object\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_vars = ['popul', 'TVnews', 'age', 'educ', 'income']\n",
      "y_full, X_full = patsy.dmatrices(\"vote ~ \" + ' + '.join(my_vars), data=data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_model = sm.Logit(y_full, X_full).fit(disp=0)\n",
      "full_log_like = full_model.llf\n",
      "full_model.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>Logit Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th>       <td>vote</td>       <th>  No. Observations:  </th>  <td>   944</td>  \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   938</td>  \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     5</td>  \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>          <td>Sat, 10 May 2014</td> <th>  Pseudo R-squ.:     </th>  <td>0.03647</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>              <td>22:18:03</td>     <th>  Log-Likelihood:    </th> <td> -617.66</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -641.05</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>6.349e-09</td>\n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Intercept</th> <td>   -2.0218</td> <td>    0.353</td> <td>   -5.720</td> <td> 0.000</td> <td>   -2.715    -1.329</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>popul</th>     <td>   -0.0001</td> <td> 7.29e-05</td> <td>   -1.989</td> <td> 0.047</td> <td>   -0.000 -2.12e-06</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>TVnews</th>    <td>   -0.0278</td> <td>    0.028</td> <td>   -1.000</td> <td> 0.317</td> <td>   -0.082     0.027</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>age</th>       <td>    0.0113</td> <td>    0.005</td> <td>    2.437</td> <td> 0.015</td> <td>    0.002     0.020</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>educ</th>      <td>    0.0347</td> <td>    0.046</td> <td>    0.755</td> <td> 0.450</td> <td>   -0.055     0.125</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>income</th>    <td>    0.0683</td> <td>    0.013</td> <td>    5.189</td> <td> 0.000</td> <td>    0.043     0.094</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 304,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                           Logit Regression Results                           \n",
        "==============================================================================\n",
        "Dep. Variable:                   vote   No. Observations:                  944\n",
        "Model:                          Logit   Df Residuals:                      938\n",
        "Method:                           MLE   Df Model:                            5\n",
        "Date:                Sat, 10 May 2014   Pseudo R-squ.:                 0.03647\n",
        "Time:                        22:18:03   Log-Likelihood:                -617.66\n",
        "converged:                       True   LL-Null:                       -641.05\n",
        "                                        LLR p-value:                 6.349e-09\n",
        "==============================================================================\n",
        "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept     -2.0218      0.353     -5.720      0.000        -2.715    -1.329\n",
        "popul         -0.0001   7.29e-05     -1.989      0.047        -0.000 -2.12e-06\n",
        "TVnews        -0.0278      0.028     -1.000      0.317        -0.082     0.027\n",
        "age            0.0113      0.005      2.437      0.015         0.002     0.020\n",
        "educ           0.0347      0.046      0.755      0.450        -0.055     0.125\n",
        "income         0.0683      0.013      5.189      0.000         0.043     0.094\n",
        "==============================================================================\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 304
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the likelihood ratio of each by re-estimating betas from scratch. This is our ground truth."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(my_vars)):\n",
      "    y, X = patsy.dmatrices(\"vote ~ \" + ' + '.join(my_vars[:i] + my_vars[i+1:]), data=data)\n",
      "    model = sm.Logit(y, X).fit(disp=0)\n",
      "    print \"Remove {0} to get {1} log diff of {2}\".format(my_vars[i], model.llf, full_log_like - model.llf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remove popul to get -619.916226021 log diff of 2.25180249346\n",
        "Remove TVnews to get -618.165450431 log diff of 0.501026903773\n",
        "Remove age to get -620.645955773 log diff of 2.98153224635\n",
        "Remove educ to get -617.949484994 log diff of 0.28506146678\n",
        "Remove income to get -632.165782034 log diff of 14.5013585065\n"
       ]
      }
     ],
     "prompt_number": 358
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just to verify we can get the correct answer outselves:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = np.dot(X_full, full_model.params)\n",
      "yy = y_full.copy()\n",
      "yy[yy==0] = -1\n",
      "print -np.log(1+np.exp(pred* -1*yy.reshape(-1))).sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-617.664423527\n"
       ]
      }
     ],
     "prompt_number": 306
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimate the log-likelihood using subtractive method: we use the original betas but zero out one variable.\n",
      "There are rather large increases in the log-likelihood ratio compared to the ground truth method, especially for\n",
      "variables with at least one correlation that is significant. This makes sense: a model without a correlated variable would have extra weight on any betas that correspond to correlated features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(my_vars)):\n",
      "    X = X_full.copy()\n",
      "    X[:, i+1] = 0\n",
      "    pred = np.dot(X, full_model.params)\n",
      "    new_log_like = -np.log(1+np.exp(pred* -1*yy.reshape(-1))).sum()\n",
      "    print \"Remove {0} to get {1} diff of {2}\".format(my_vars[i], new_log_like, full_log_like - new_log_like)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remove popul to get -620.104624299 diff of 2.44020077219\n",
        "Remove TVnews to get -619.45057184 diff of 1.78614831254\n",
        "Remove age to get -651.475959519 diff of 33.811535992\n",
        "Remove educ to get -620.773265973 diff of 3.10884244548\n",
        "Remove income to get -766.026617695 diff of 148.362194167\n"
       ]
      }
     ],
     "prompt_number": 307
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[my_vars].corr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>popul</th>\n",
        "      <th>TVnews</th>\n",
        "      <th>age</th>\n",
        "      <th>educ</th>\n",
        "      <th>income</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>popul</th>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.020457</td>\n",
        "      <td>-0.043720</td>\n",
        "      <td>-0.003869</td>\n",
        "      <td>-0.018009</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>TVnews</th>\n",
        "      <td> 0.020457</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.408784</td>\n",
        "      <td>-0.058109</td>\n",
        "      <td>-0.062568</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>age</th>\n",
        "      <td>-0.043720</td>\n",
        "      <td> 0.408784</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td>-0.158841</td>\n",
        "      <td>-0.078499</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>educ</th>\n",
        "      <td>-0.003869</td>\n",
        "      <td>-0.058109</td>\n",
        "      <td>-0.158841</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.372771</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>income</th>\n",
        "      <td>-0.018009</td>\n",
        "      <td>-0.062568</td>\n",
        "      <td>-0.078499</td>\n",
        "      <td> 0.372771</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 289,
       "text": [
        "           popul    TVnews       age      educ    income\n",
        "popul   1.000000  0.020457 -0.043720 -0.003869 -0.018009\n",
        "TVnews  0.020457  1.000000  0.408784 -0.058109 -0.062568\n",
        "age    -0.043720  0.408784  1.000000 -0.158841 -0.078499\n",
        "educ   -0.003869 -0.058109 -0.158841  1.000000  0.372771\n",
        "income -0.018009 -0.062568 -0.078499  0.372771  1.000000\n",
        "\n",
        "[5 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 289
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.stats import pearsonr\n",
      "print pearsonr(data.popul, data.age)\n",
      "print pearsonr(data.TVnews, data.age)\n",
      "print pearsonr(data.educ, data.age)\n",
      "print pearsonr(data.educ, data.income)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(-0.043719647475027192, 0.17955387578706439)\n",
        "(0.40878425974859989, 2.5070154264168333e-39)\n",
        "(-0.15884065995417987, 9.3418189665825033e-07)\n",
        "(0.37277142870064001, 1.7253837688422475e-32)\n"
       ]
      }
     ],
     "prompt_number": 366
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's introduce a correction for a particular correlation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "educ_m = smf.ols(\"educ ~ income -1\", data=data).fit()\n",
      "educ_m.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>OLS Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th>          <td>educ</td>       <th>  R-squared:         </th> <td>   0.863</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.862</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5917.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>             <td>Sun, 11 May 2014</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>                 <td>09:29:59</td>     <th>  Log-Likelihood:    </th> <td> -1890.9</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>No. Observations:</th>      <td>   944</td>      <th>  AIC:               </th> <td>   3784.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Residuals:</th>          <td>   943</td>      <th>  BIC:               </th> <td>   3789.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>income</th> <td>    0.2584</td> <td>    0.003</td> <td>   76.923</td> <td> 0.000</td> <td>    0.252     0.265</td>\n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "  <th>Omnibus:</th>       <td>15.842</td> <th>  Durbin-Watson:     </th> <td>   1.324</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  16.286</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Skew:</th>          <td> 0.310</td> <th>  Prob(JB):          </th> <td>0.000291</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Kurtosis:</th>      <td> 2.828</td> <th>  Cond. No.          </th> <td>    1.00</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 327,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                   educ   R-squared:                       0.863\n",
        "Model:                            OLS   Adj. R-squared:                  0.862\n",
        "Method:                 Least Squares   F-statistic:                     5917.\n",
        "Date:                Sun, 11 May 2014   Prob (F-statistic):               0.00\n",
        "Time:                        09:29:59   Log-Likelihood:                -1890.9\n",
        "No. Observations:                 944   AIC:                             3784.\n",
        "Df Residuals:                     943   BIC:                             3789.\n",
        "Df Model:                           1                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "income         0.2584      0.003     76.923      0.000         0.252     0.265\n",
        "==============================================================================\n",
        "Omnibus:                       15.842   Durbin-Watson:                   1.324\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               16.286\n",
        "Skew:                           0.310   Prob(JB):                     0.000291\n",
        "Kurtosis:                       2.828   Cond. No.                         1.00\n",
        "==============================================================================\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 327
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $educ \\approx 0.2584 * income + \\epsilon$ then change the beta for education and remove income"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = X_full.copy()\n",
      "params = full_model.params.copy()\n",
      "params[-2] += 0.2584 \n",
      "X[:,-1] = 0\n",
      "pred = np.dot(X, params)\n",
      "new_log_like = -np.log(1+np.exp(pred* -1*yy.reshape(-1))).sum()\n",
      "print \"Remove {0} to get {1} diff of {2}\".format(my_vars[-1], new_log_like, full_log_like - new_log_like)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remove income to get -640.547663853 diff of 22.8832403262\n"
       ]
      }
     ],
     "prompt_number": 367
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pop_m = smf.ols(\"TVnews ~ age - 1\", data=data).fit()\n",
      "pop_m.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>OLS Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th>         <td>TVnews</td>      <th>  R-squared:         </th> <td>   0.715</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.715</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2365.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>             <td>Sun, 11 May 2014</td> <th>  Prob (F-statistic):</th> <td>2.95e-259</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>                 <td>21:17:34</td>     <th>  Log-Likelihood:    </th> <td> -2185.3</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>No. Observations:</th>      <td>   944</td>      <th>  AIC:               </th> <td>   4373.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Residuals:</th>          <td>   943</td>      <th>  BIC:               </th> <td>   4377.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>age</th> <td>    0.0779</td> <td>    0.002</td> <td>   48.635</td> <td> 0.000</td> <td>    0.075     0.081</td>\n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "  <th>Omnibus:</th>       <td>65.967</td> <th>  Durbin-Watson:     </th> <td>   2.088</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  23.377</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Skew:</th>          <td>-0.032</td> <th>  Prob(JB):          </th> <td>8.39e-06</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Kurtosis:</th>      <td> 2.232</td> <th>  Cond. No.          </th> <td>    1.00</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 368,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                 TVnews   R-squared:                       0.715\n",
        "Model:                            OLS   Adj. R-squared:                  0.715\n",
        "Method:                 Least Squares   F-statistic:                     2365.\n",
        "Date:                Sun, 11 May 2014   Prob (F-statistic):          2.95e-259\n",
        "Time:                        21:17:34   Log-Likelihood:                -2185.3\n",
        "No. Observations:                 944   AIC:                             4373.\n",
        "Df Residuals:                     943   BIC:                             4377.\n",
        "Df Model:                           1                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "age            0.0779      0.002     48.635      0.000         0.075     0.081\n",
        "==============================================================================\n",
        "Omnibus:                       65.967   Durbin-Watson:                   2.088\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.377\n",
        "Skew:                          -0.032   Prob(JB):                     8.39e-06\n",
        "Kurtosis:                       2.232   Cond. No.                         1.00\n",
        "==============================================================================\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 368
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = X_full.copy()\n",
      "params = full_model.params.copy()\n",
      "params[2] += 0.0779 \n",
      "X[:,-3] = 0\n",
      "pred = np.dot(X, params)\n",
      "new_log_like = -np.log(1+np.exp(pred* -1*yy.reshape(-1))).sum()\n",
      "print \"Remove {0} to get {1} diff of {2}\".format(my_vars[-3], new_log_like, full_log_like - new_log_like)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remove age to get -629.098684238 diff of 11.4342607112\n"
       ]
      }
     ],
     "prompt_number": 372
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: \n",
      "\n",
      "This only matters (for me) if I have a cheap way to estimate beta. The cost of the full model can be born once, and the cost of computing all correlations can be done in one map-reduce cycle (compute all SS, compute all pairs) though I will probably use two for ease of programming. \n",
      "\n",
      "The good news is that computing the correction parameters from original betas and correlations is not hard:\n",
      "\n",
      "$$ \\rho_{xy} = \\frac{SS_{xy}}{\\sqrt{SS_x SS_y}} $$\n",
      "and $ \\beta_x $ in $y ~ x$ is equal to \n",
      "$$ \\frac{SS_{xy}}{SS_x} $$\n",
      "\n",
      "If I pre-center all of my variables to zero mean (so the intercept in $y ~ x$ is zero) then I have a cheap way to compute the pairwise betas, and I can perform my correction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I should add the word of caution: don't attribute any statistical meaning to the betas. I am assuming a lot of things here:\n",
      "\n",
      "* The regression in real life will be regularized with LASSO due to number of parameters. I would use $l_0$ only but that isn't robust as a feature selection mechanism [Murphy]. _I assume that removed parameter $x_i$ would not cause another parameter to enter the model._ In principle, I could force in anything correlated with $x_i$ by running correlation over all features.\n",
      "* I assume that a rough ordering is all that matters in my feature detection: if I can get the top 20 features then run accurate log-likelihood estimation, I can identify the top 10 or so robustly (if needed...)\n",
      "* I assume that I won't need to define order 2 models as corrections. Given my moderate knowledge of learning in massive systems, this seems like a fair-ish assumption.\n",
      "\n",
      "But the point is to define a method that minimizes map-reduce cycles, which is a proxy for minimizing data access. This might do that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 356
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}